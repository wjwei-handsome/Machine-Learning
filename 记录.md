# 								机器学习记录

# 零.写在前面

## 由于课题需要，再加上个人预感将来基因组学必将和机器学习产生深刻的联系，所以开始在2020疫情期间自学机器学习，资料均来自网络，参考书包括但不限于《机器学习实战--基于Scikit-Learn和TensoFlow》、《机器学习-周志华》、《统计学习方法-李航》

# 一.开始

## 0.配置vscode远程开发环境

### 	1.在MacBook上由于之前已经产生了密钥，在～/.ssh/id_rsa.pub

### 	2.将上一步的密钥复制到服务器的~/.ssh/authorized_keys

### 	3.给一定的权限并重启服务

```shell
sudo chmod 600 ~/.ssh
sudo chmod 700 ~/.ssh/authorized_keys
sudo /bin/systemctl restart sshd.service
```

### 	4.配置MacBook的config，在vscode中的Romote SSH插件中编辑config文件

```
Host 阿里云
  HostName 47.115.36.127
  Port 22
  User root
  IdentityFile "/Users/wjwei/.ssh/id_rsa"
```

### 	5.在vscode中可以直接免密码登陆并配置相关环境(git)

## 1.克隆项目

```shell
git clone https://github.com/ageron/handson-ml.git	
```

将参考书上的项目克隆到阿里云服务器上，因为在root服务器上配置环境更加方便一点

## 2.安装Anaconda并配置环境

```shell
bash Anaconda3-2020.02-Linux-x86_64.sh
```

```shell
cd handson-ml
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ #更换为清华大学的国内源加快速度
conda config --set show_channel_urls yes
conda env create -f environment.yml  #创建一个独立的环境
conda activate tf1 #激活创建的环境，名称叫tf1
```

## 3.第一个小例子

### 根据几个国家的幸福指数与GDP，来建立一个线性模型，并预测某一个国家的幸福指数。数据集已经在刚才clone的项目中。

```python
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn
import sklearn.linear_model

def prepare_country_stats(oecd_bli, gdp_per_capita): #将生活满意度和GDP合并为一个Dataframe
    oecd_bli = oecd_bli[oecd_bli["INEQUALITY"]=="TOT"] #根据列值筛选
    oecd_bli = oecd_bli.pivot(index="Country", columns="Indicator", values="Value") #创建一个派生数据集
    gdp_per_capita.rename(columns={"2015": "GDP per capita"}, inplace=True)
    gdp_per_capita.set_index("Country", inplace=True)
    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,
                                  left_index=True, right_index=True)
    full_country_stats.sort_values(by="GDP per capita", inplace=True)
    remove_indices = [0, 1, 6, 8, 33, 34, 35]
    keep_indices = list(set(range(36)) - set(remove_indices))
    return full_country_stats[["GDP per capita", 'Life satisfaction']].iloc[keep_indices]

datapath = os.path.join("datasets", "lifesat", "")

oecd_bli = pd.read_csv(datapath + "oecd_bli_2015.csv", thousands=',')
gdp_per_capita = pd.read_csv(datapath + "gdp_per_capita.csv",thousands=',',delimiter='\t',
                             encoding='latin1', na_values="n/a")
country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)

X = np.c_[country_stats["GDP per capita"]]
y = np.c_[country_stats["Life satisfaction"]]

model = sklearn.linear_model.LinearRegression()  #线性回归模型
model.fit(X, y)
X_new = [[22587]]
print(model.predict(X_new)) #预测
##可以得到这个简单线性模型的两个参数
print(model.intercept_) #斜率
print(mode.coef_) #截距
print(model.score(X,y)) #R^2 回归系数，相当于精确度
```

#### 这里这几个地方需要注意：

| DataFrame.pivot                             | 根据主数据集，传递三个必须的参数index、columns、values来创建一个派生的数据集 |
| :------------------------------------------ | ------------------------------------------------------------ |
| **sklearn.linear_model.LinearRegression**() | **选择线性回归模型**                                         |
| **model.fit(X, y)**                         | **根据挑选的模型进行训练**                                   |
| **intercept_ coef_**                        | **斜率与截距**                                               |
| **model.score(X,y)**                        | **评估精确度**                                               |

#### 这个小例子使用的是线性回归模型，也可以替换为k-近邻回归算法（之后学习）。

```python
import sklearn.neighbors
model2 = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)
model2.fit(X,y)
print(model2.predict(X_new))
```

#### 当然两个模型预测的结果是由出入的，简而言之，机器学习大概经历四个步骤：

##### 1.准备数据 2.选择合适的模型 3.进行训练 4.进行预测

## 4.第二个例子

### 1.读取数据csv

```python
import pandas as pd
housing=pd.read_csv('datasets/housing/housing.csv')
```

### 2.查看数据结构

```python
housing.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 10 columns):
 #   Column              Non-Null Count  Dtype  
---  ------              --------------  -----  
 0   longitude           20640 non-null  float64
 1   latitude            20640 non-null  float64
 2   housing_median_age  20640 non-null  float64
 3   total_rooms         20640 non-null  float64
 4   total_bedrooms      20433 non-null  float64
 5   population          20640 non-null  float64
 6   households          20640 non-null  float64
 7   median_income       20640 non-null  float64
 8   median_house_value  20640 non-null  float64
 9   ocean_proximity     20640 non-null  object 
dtypes: float64(9), object(1)
memory usage: 1.6+ MB
```

这里可以看到一共有10列，20640行，但是其中total_bedrooms并不是所有数据都是都有，有许多缺失的数据。

除了ocean_proximity外，其他所有属性都是都是float浮点数，而例外是object属性，可以通过**value_counts**函数来查看有多少种分类

```python
print(housing["ocean_proximity"].value_counts())
<1H OCEAN     9136
INLAND        6551
NEAR OCEAN    2658
NEAR BAY      2290
ISLAND           5
Name: ocean_proximity, dtype: int64
```

在**value_counts**函数中，有这么几个参数值得注意

| 参数          | 说明                                                         |
| ------------- | ------------------------------------------------------------ |
| **normalize** | If True then the object returned will contain the relative frequencies of the unique values. |
| **sort**      | Sort by frequencies.                                         |
| **ascending** | Sort in ascending order.                                     |
| **bins**      | Rather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data. |
| **dropna**    | Don’t include counts of NaN.                                 |

如果加上**normalize**参数：

```
<1H OCEAN     0.442636
INLAND        0.317393
NEAR OCEAN    0.128779
NEAR BAY      0.110950
ISLAND        0.000242
Name: ocean_proximity, dtype: float64
```

### 3.查看数值属性

```python
print(housing.describe())
          longitude      latitude  housing_median_age   total_rooms  total_bedrooms    population    households  median_income  median_house_value
count  20640.000000  20640.000000        20640.000000  20640.000000    20433.000000  20640.000000  20640.000000   20640.000000        20640.000000
mean    -119.569704     35.631861           28.639486   2635.763081      537.870553   1425.476744    499.539680       3.870671       206855.816909
std        2.003532      2.135952           12.585558   2181.615252      421.385070   1132.462122    382.329753       1.899822       115395.615874
min     -124.350000     32.540000            1.000000      2.000000        1.000000      3.000000      1.000000       0.499900        14999.000000
25%     -121.800000     33.930000           18.000000   1447.750000      296.000000    787.000000    280.000000       2.563400       119600.000000
50%     -118.490000     34.260000           29.000000   2127.000000      435.000000   1166.000000    409.000000       3.534800       179700.000000
75%     -118.010000     37.710000           37.000000   3148.000000      647.000000   1725.000000    605.00
```

### 4.分离出测试集，根据抽样分布的结果

这里使用sklearn的StratifiedShuffleSplit函数，先来看官方的例子：

```python
from sklearn.model_selection import StratifiedShuffleSplit
import numpy as np
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4],[1, 2],
              [3, 4], [1, 2], [3, 4],[4,5],[4,5]])#训练数据集10*2
print(X)
y = np.array([0, 0, 1, 1, 0, 0, 1, 1, 2, 2])#类别数据集10*1
print(y)

ss=StratifiedShuffleSplit(n_splits=2,test_size=0.25,random_state=42)#分成5组，测试比例为0.25

for train_index, test_index in ss.split(X, y):
    print("TRAIN_INDEX:", train_index, "TEST_INDEX:", test_index)#获得索引值
    X_train, X_test = X[train_index], X[test_index]#训练集对应的值
    y_train, y_test = y[train_index], y[test_index]#类别集对应的值
    print("X_train:",X_train)
    print("y_train:",y_train)

运行结果：
[[1 2]
 [3 4]
 [1 2]
 [3 4]
 [1 2]
 [3 4]
 [1 2]
 [3 4]
 [4 5]
 [4 5]]
[0 0 1 1 0 0 1 1 2 2]
TRAIN_INDEX: [3 9 1 7 2 0 4] TEST_INDEX: [5 6 8]
X_train: [[3 4]
 [4 5]
 [3 4]
 [3 4]
 [1 2]
 [1 2]
 [1 2]]
y_train: [1 2 0 1 1 0 0]
TRAIN_INDEX: [8 5 0 7 6 2 4] TEST_INDEX: [3 1 9]
X_train: [[4 5]
 [3 4]
 [1 2]
 [3 4]
 [1 2]
 [1 2]
 [1 2]]
y_train: [2 0 0 1 1 1 0]
```

从示例可以看出，分类标准就是确保标签的比例一致，两次输出标签0,1,2的比例一致。其实数据量大的时候，还保证了这些标签在在取样后的比例和抽样前的一致。极而言之就是0,1,2在y中所站的比例和0,1,2在y_train中的比例一致。在我们的例子中，我们需要挑选一个具有代表性的属性。

```python
housing['income_cat']=np.ceil(housing['median_income']/1.5)
housing['income_cat'].where(housing['income_cat'] < 5,5.0,inplace=True) #将大于5的类别合并为5
type(housing)
from sklearn.model_selection import StratifiedShuffleSplit
split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=41)
split.get_n_splits()
for train_index,test_index in split.split(housing,housing['income_cat']):   
    start_train_set=housing.loc[train_index]
    start_test_set=housing.loc[test_index]
```

分离完成后，删掉刚才的分类依据属性：

```python
for set in (start_train_set,start_test_set):
    set.drop(['income_cat'],axis=1,inplace=True)
housing=start_train_set.copy()
len(start_train_set)/len(start_test_set)
```

### 5.计算属性之间的相关系数（皮尔森）

```python
corr_matrix=housing.corr()
corr_matrix
                    longitude  latitude  housing_median_age  total_rooms  total_bedrooms  population  households  median_income  median_house_value
longitude            1.000000 -0.924208           -0.109516     0.049306        0.074237    0.105816    0.059781      -0.019568           -0.049897
latitude            -0.924208  1.000000            0.011499    -0.038834       -0.068830   -0.112147   -0.072910      -0.076904           -0.141620
housing_median_age  -0.109516  0.011499            1.000000    -0.363013       -0.323297   -0.300212   -0.305806      -0.114760            0.111686
total_rooms          0.049306 -0.038834           -0.363013     1.000000        0.931339    0.857320    0.918272       0.196804            0.131987
total_bedrooms       0.074237 -0.068830           -0.323297     0.931339        1.000000    0.877830    0.978793      -0.007924            0.046671
population           0.105816 -0.112147           -0.300212     0.857320        0.877830    1.000000    0.908247       0.004892           -0.024677
households           0.059781 -0.072910           -0.305806     0.918272        0.978793    0.908247    1.000000       0.012923            0.063285
median_income       -0.019568 -0.076904           -0.114760     0.196804       -0.007924    0.004892    0.012923       1.000000            0.687668
median_house_value  -0.049897 -0.141620            0.111686     0.131987        0.046671   -0.024677    0.063285       0.687668            1.000000
```

### 6.将预测的属性与特征标签分离开

```python
housing=housing.drop('median_house_value',axis=1)
housing_labels=start_train_set['median_house_value'].copy()
```

### 7.处理缺失值

这里利用sklearn的imputer

```python
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='median')#用中位数补全的
housing_num=housing.drop('ocean_proximity',axis=1)
imputer.fit(housing_num)
imputer.statistics_
housing_num.median().values
X=imputer.transform(housing_num)
housing_tr=pd.DataFrame(X,columns=housing_num.columns)
housing_tr.info()
```

### 8.处理文本和分类属性


